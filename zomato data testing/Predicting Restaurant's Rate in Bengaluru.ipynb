{"cells":[{"metadata":{"toc":true},"cell_type":"markdown","source":"<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Libraries-and-Functions\" data-toc-modified-id=\"Libraries-and-Functions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Libraries and Functions</a></span></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Understanding-the-Data\" data-toc-modified-id=\"Understanding-the-Data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Understanding the Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Types\" data-toc-modified-id=\"Data-Types-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Data Types</a></span></li><li><span><a href=\"#Null-Data\" data-toc-modified-id=\"Null-Data-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Null Data</a></span></li><li><span><a href=\"#Outliers\" data-toc-modified-id=\"Outliers-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Outliers</a></span></li></ul></li><li><span><a href=\"#Graphical-Analysis\" data-toc-modified-id=\"Graphical-Analysis-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Graphical Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Top-Restaurants-by-Rating\" data-toc-modified-id=\"Top-Restaurants-by-Rating-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Top Restaurants by Rating</a></span></li><li><span><a href=\"#Cost-and-Rate-of-Restaurants\" data-toc-modified-id=\"Cost-and-Rate-of-Restaurants-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Cost and Rate of Restaurants</a></span></li><li><span><a href=\"#Online-Order-and-Book-Table-Services\" data-toc-modified-id=\"Online-Order-and-Book-Table-Services-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Online Order and Book Table Services</a></span></li><li><span><a href=\"#Restaurants-Type\" data-toc-modified-id=\"Restaurants-Type-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>Restaurants Type</a></span></li><li><span><a href=\"#Location\" data-toc-modified-id=\"Location-2.2.5\"><span class=\"toc-item-num\">2.2.5&nbsp;&nbsp;</span>Location</a></span></li></ul></li></ul></li><li><span><a href=\"#Explained-Data-Prep\" data-toc-modified-id=\"Explained-Data-Prep-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Explained Data Prep</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classes-for-Pipeline\" data-toc-modified-id=\"Classes-for-Pipeline-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Classes for Pipeline</a></span></li><li><span><a href=\"#Complete-Pipeline\" data-toc-modified-id=\"Complete-Pipeline-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Complete Pipeline</a></span></li></ul></li><li><span><a href=\"#Predicting-Restaurant's-Rate\" data-toc-modified-id=\"Predicting-Restaurant's-Rate-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Predicting Restaurant's Rate</a></span><ul class=\"toc-item\"><li><span><a href=\"#Regularization\" data-toc-modified-id=\"Regularization-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Regularization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ridge\" data-toc-modified-id=\"Ridge-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Ridge</a></span></li><li><span><a href=\"#Lasso\" data-toc-modified-id=\"Lasso-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Lasso</a></span></li><li><span><a href=\"#Elastic-Net\" data-toc-modified-id=\"Elastic-Net-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Elastic Net</a></span></li></ul></li><li><span><a href=\"#Random-Forest-Regressor\" data-toc-modified-id=\"Random-Forest-Regressor-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Random Forest Regressor</a></span></li><li><span><a href=\"#Rate-of-New-Restaurants\" data-toc-modified-id=\"Rate-of-New-Restaurants-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Rate of New Restaurants</a></span></li></ul></li><li><span><a href=\"#Classifying-Customer-Satisfaction\" data-toc-modified-id=\"Classifying-Customer-Satisfaction-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Classifying Customer Satisfaction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classification-Performance\" data-toc-modified-id=\"Classification-Performance-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Classification Performance</a></span></li><li><span><a href=\"#Random-Forest-Classifier\" data-toc-modified-id=\"Random-Forest-Classifier-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Random Forest Classifier</a></span></li><li><span><a href=\"#Class-of-New-Restaurants\" data-toc-modified-id=\"Class-of-New-Restaurants-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Class of New Restaurants</a></span></li></ul></li></ul></div>"},{"metadata":{},"cell_type":"markdown","source":"Welcome to my kernel! This time we have a really funny and challenging task involving data from restaurants in Bengaluru, a city of India well known by its food culture and variety. Here, we will go through data visualization to get insights from the data.\n\nAfter, we will train a model to predict the rate given by the users for restaurants. This should be a really interesting analysis and I hope to present you good conclusions about it. If you like, please **upvote** this kernel!\n\nEnglish isn't my mother language, sorry for any mistakes."},{"metadata":{},"cell_type":"markdown","source":"# Libraries and Functions"},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:05.041777Z","end_time":"2019-08-14T18:18:06.325794Z"},"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV, learning_curve\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import mean_squared_error, accuracy_score\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:31:48.863390Z","end_time":"2019-08-14T18:31:48.943432Z"},"trusted":true},"cell_type":"code","source":"def format_spines(ax, right_border=True):\n    \"\"\"\n    This is a usefull function used for make plots more beautiful\n    \n    Input:\n        ax: matplotlib axis\n        right_border: boolean flag for deciding or not to plot the right border of axis\n    \"\"\"    \n    ax.spines['bottom'].set_color('#CCCCCC')\n    ax.spines['left'].set_color('#CCCCCC')\n    ax.spines['top'].set_visible(False)\n    if right_border:\n        ax.spines['right'].set_color('#CCCCCC')\n    else:\n        ax.spines['right'].set_color('#FFFFFF')\n    ax.patch.set_facecolor('#FFFFFF')\n    \ndef donut_plot(col, ax, df, labels, text='', colors=['navy', 'crimson']):\n    \"\"\"\n    This function plots a customized donut chart of the target column (binary task)\n    \n    Input:\n        col: target column of a binary classification task\n        ax: matplotlib axis\n        df: DataFrame object with the data\n        text: the text to be plotted on the center of the donut chart\n        colors: list of two colors used to identify the target class\n        labels: list of labels to describe the target class\n    Output:\n        a customized donut chart\n    \"\"\"\n    sizes = df[col].value_counts().values\n    center_circle = plt.Circle((0,0), 0.80, color='white')\n    ax.pie((sizes[0], sizes[1]), labels=labels, colors=colors, autopct='%1.2f%%')\n    ax.add_artist(center_circle)\n    kwargs = dict(size=20, fontweight='bold', va='center')\n    ax.text(0, 0, text, ha='center', **kwargs)\n    \ndef create_dataset():\n    \"\"\"\n    This functions creates a pandas DataFrame for receiving performance analysis\n    \n    Returns:\n        An empty pandas DataFrame object with regression metrics to be evaluated\n    \"\"\"\n    attributes = ['model', 'rmse_train', 'rmse_cv', 'rmse_test', 'total_time']\n    model_performance = pd.DataFrame({})\n    for col in attributes:\n        model_performance[col] = []\n    return model_performance\n\ndef model_results(models, X_train, y_train, X_test, y_test, df_performance, cv=10, \n                  scoring='neg_mean_squared_error'):\n    \"\"\"\n    This function brings up a full model evaluation and saves it in a DataFrame object\n    \n    Input:\n        model: a dictionary with regression models\n        X_train, y_train, X_test, y_test: data to be evaluated\n        df_performance: an empty dataframe (generated by crate_dataset() function)\n        cv: cross validation k folds\n        \n    Returns:\n        A DataFrame object with all metrics from all models on classifier's dictionary.\n    \"\"\"\n    for name, model in models.items():\n        t0 = time.time()\n        model.fit(X_train, y_train)\n        train_pred = model.predict(X_train)\n        train_mse = mean_squared_error(y_train, train_pred)\n        train_rmse = np.sqrt(train_mse)\n\n        train_cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=scoring)\n        train_cv_rmse = np.sqrt(-train_cv_scores).mean()\n\n        test_pred = model.predict(X_test)\n        test_mse = mean_squared_error(y_test, test_pred)\n        test_rmse = np.sqrt(test_mse)\n        t1 = time.time()\n        delta_time = t1-t0\n        model_name = model.__class__.__name__\n\n        performances = {}\n        performances['model'] = model_name\n        performances['rmse_train'] = round(train_rmse, 4)\n        performances['rmse_cv'] = round(train_cv_rmse, 4)\n        performances['rmse_test'] = round(test_rmse, 4)\n        performances['total_time'] = round(delta_time, 3)\n        df_performance = df_performance.append(performances, ignore_index=True)\n        \n    return df_performance\n\ndef create_empty_classification_dataset():\n    \"\"\"\n    This functions creates a pandas DataFrame for receiving performance analysis on a classification task\n    \n    Returns:\n        An empty pandas DataFrame object with classification metrics to be evaluated\n    \"\"\"\n    attributes = ['acc', 'total_time']\n    model_performance = pd.DataFrame({})\n    for col in attributes:\n        model_performance[col] = []\n        \n    return model_performance\n\ndef model_analysis_class_task(models, X, y, X_test, y_test, df_performance, cv=5, train=True):\n    \"\"\"\n    This function brings up a full model evaluation and saves it in a DataFrame object\n    \n    Input:\n        model: a classification model already trained\n        X, y: data to be evaluated and the target class\n        df_performance: an empty dataframe (generated by crate_dataset() function)\n        cv: cross validation k folds\n        train: a boolean flag for telling it it's a train cv evaluation or a test evaluation\n    Returns:\n        A DataFrame object with all metrics from all models on classifier's dictionary.\n    \"\"\"\n    idx_models = []\n    for model_name, model in models.items():\n        # Accuracy, precision, recall and f1_score on training set using cv\n        t0_cv = time.time()\n        acc = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()\n        # Time spent on cross_validation prediction\n        t1_cv = time.time()\n        delta_time_cv = t1_cv-t0_cv\n\n        # Evaluation using the test set\n        t0_test = time.time()\n        y_pred_test = model.predict(X_test)\n        acc_test = accuracy_score(y_test, y_pred_test)\n        # Time spent on test prediction\n        t1_test = time.time()\n        delta_time_test = t1_test-t0_test\n\n        # Saving on dataframe\n        performances = {}\n        performances['acc'] = round(acc, 4)\n        performances['total_time'] = round(delta_time_cv, 3)        \n        df_performance = df_performance.append(performances, ignore_index=True)\n\n        test_performances = {}\n        test_performances['acc'] = round(acc_test, 4)\n        test_performances['total_time'] = round(delta_time_test, 3)        \n        df_performance = df_performance.append(test_performances, ignore_index=True)\n\n        idx_models.append(model.__class__.__name__)\n    \n    idx = []\n    for name in idx_models:\n        idx.append(name + '_cv')\n        idx.append(name + '_test')\n    df_performance.index = idx\n    \n    return df_performance\n\ndef plot_learning_curve(model, ax, X, y, ylim=None, cv=5, n_jobs=1, \n                        train_sizes=np.linspace(.1, 1.0, 10)):\n    \"\"\"\n    This functions plots the learning curve for a model using training and cross validation data\n    \n    Input:\n        model: a classification model already trained\n        ax: matplotlib axis\n        X: training set\n        y: training label\n        y_lim: y axis limit\n        cv: cross validation k folds\n        train_sizes: bucket or bins to split the learning curve\n    Output:\n        Learning curves for models in the trained_models dictionary\n    \"\"\"\n    # Returning metrics with sklearn's learning_curve function\n    train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=cv, n_jobs=n_jobs,\n                                                           train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    \n    # Ploting the learning curve and standard deviation bin\n    ax.fill_between(train_sizes, (train_scores_mean-train_scores_std), (train_scores_mean+train_scores_std),\n                    alpha=0.1, color='blue')\n    ax.fill_between(train_sizes, (test_scores_mean-test_scores_std), (test_scores_mean+test_scores_std),\n                    alpha=0.1, color='crimson')\n    ax.plot(train_sizes, train_scores_mean, 'o-', color='navy', label='Training score')\n    ax.plot(train_sizes, test_scores_mean, 'o-', color='red', label='Cross-Validation score')\n    \n    # Plot configuration\n    ax.set_title(f'{model.__class__.__name__} Learning Curve', size=14)\n    ax.set_xlabel('Training size (m)')\n    ax.set_ylabel('Score')\n    ax.grid(True)\n    ax.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"The objective of this session is to apply an exploratory analysis and go through the following topics:\n\n* Understand the data;\n* Apply transformations when required;\n* Visualize restaurant's informations and look for relationship between attributes;\n\n\n* **The graphical exploration will include:**\n    * Top Restaurants by rating;\n    * The approx cost (for two people) and rate of restaurants;\n    * Correlation between cost and rate;\n    * The influence of online_order and book_table attributes;\n    * Cost and rate by type of restaurants;\n    * Cost and rate of restaurants by localization (city)\n    * Look for relationship between attributes."},{"metadata":{},"cell_type":"markdown","source":"## Understanding the Data"},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:06.369399Z","end_time":"2019-08-14T18:18:11.689409Z"},"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/zomato.csv')\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:11.691665Z","end_time":"2019-08-14T18:18:11.695284Z"},"trusted":true},"cell_type":"code","source":"print(f'This dataset has {df.shape[0]} rows and {df.shape[1]} columns.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Types"},{"metadata":{},"cell_type":"markdown","source":"The first thing we will do about understanding the dataset is looking on its data types"},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:11.697525Z","end_time":"2019-08-14T18:18:11.703742Z"},"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's do some transformation here:\n\n* **approx_cost(for two people):**\n    - Change the data type from object to float;\n    \n\n* **rate:**\n    - Let's eliminate the \"/5\" text and change data type from object to float"},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:11.705232Z","end_time":"2019-08-14T18:18:11.770411Z"},"trusted":true},"cell_type":"code","source":"# Changing data type from approx_cost column\ndf['approx_cost(for two people)'] = df['approx_cost(for two people)'].astype(str)\ndf['approx_cost(for two people)'] = df['approx_cost(for two people)'].apply(lambda x: x.replace(',', '.'))\ndf['approx_cost(for two people)'] = df['approx_cost(for two people)'].astype(float)\nprint(f'{type(df[\"approx_cost(for two people)\"][0])}')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:11.773879Z","end_time":"2019-08-14T18:18:11.896070Z"},"trusted":true},"cell_type":"code","source":"# Transforming rate column\ndf['rate_transformed'] = df['rate'].astype(str)\ndf['rate_transformed'] = df['rate_transformed'].apply(lambda x: x.split('/')[0])\n\n# Dealing with instanced with 'NEW'\ndf['rate_transformed'] = df['rate_transformed'].apply(lambda x: x.replace('NEW', str(np.nan)))\ndf['rate_transformed'] = df['rate_transformed'].apply(lambda x: x.replace('-', str(np.nan)))\n\n# Changing data type\ndf['rate_transformed'] = df['rate_transformed'].astype(float)\ndf.drop(['rate'], axis=1, inplace=True)\nprint(f'{type(df[\"rate_transformed\"][0])}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Null Data"},{"metadata":{},"cell_type":"markdown","source":"Before looking to the null data we need to set a point: do we really need all of features in this data for our analysis? The answear is no! So we will create a copy from the original dataset to modify it as long as we want it to.\n\nIn the future, if we miss some feature or if we get a different idea about a different development, we can get the original data with the original attributes again."},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:11.898242Z","end_time":"2019-08-14T18:18:11.957280Z"},"trusted":true},"cell_type":"code","source":"# Selecting attributes\nbengaluru = df.copy()\nbengaluru.drop(['url', 'address', 'dish_liked', 'phone',\n                'cuisines', 'reviews_list', 'menu_item'], axis=1, inplace=True)\n\n# Looking for null data\nbengaluru.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Studying how to deal with null data in **rate** attribute"},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:11.959866Z","end_time":"2019-08-14T18:18:11.970127Z"},"trusted":true},"cell_type":"code","source":"bengaluru['rate_transformed'].describe()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:11.971618Z","end_time":"2019-08-14T18:18:12.199439Z"},"trusted":true},"cell_type":"code","source":"sns.set(style='white', palette='muted', color_codes=True)\nfig, ax = plt.subplots(figsize=(12, 5))\nsns.despine(left=True)\nsns.distplot(bengaluru['rate_transformed'], bins=30, color='navy')\nax.set_title(\"Restaurant's Rate Distribution\", size=14)\nax.set_xlabel('Restaurant Rate')\nplt.setp(ax, yticks=[])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, the statistics tell us that the mean value and the median value (50%) from rate attribute are quite similar and it has a low standard deviation. Maybe there wouldn't be a crime to fill the null data in this attribute with a rate of 3.7! But, we have more than 10,000 null data and, as long as we want to see the real performance of the restaurants, we probably would be \"cheating\" saying that poor restaurants have a reasonable rate.\n\nAlso, it is possible to say that restaurants with no rate are new ones or the costumers simply don't had enough information to give it a evaluation. Before making any decision, let's study how to deal with null data in **approx_cost** attribute."},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:12.200738Z","end_time":"2019-08-14T18:18:12.209959Z"},"trusted":true},"cell_type":"code","source":"bengaluru['approx_cost(for two people)'].describe()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:12.212408Z","end_time":"2019-08-14T18:18:12.325230Z"},"trusted":true},"cell_type":"code","source":"sns.set(style='white', palette='muted', color_codes=True)\nfig, ax = plt.subplots(figsize=(12, 5))\nsns.despine(left=True)\nsns.distplot(bengaluru['approx_cost(for two people)'], bins=10, color='navy')\nax.set_title(\"Restaurant's Approx Cost Distribution\", size=14)\nax.set_xlabel('Approx Cost (for two people)')\nplt.setp(ax, yticks=[])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we have a more bumpy feature with values ranging from 1 to 950. The standard deviation for the approx cost is too high and maybe fill it with the mean or median is a mistake. The other two attributes with null data (location and rest_type) are categorical ones and maybe we could train a classification algorithm to predict them but we don't know yet how many categories they have and how difficult it would be.\n\n**Conclusions**\n\nLastly, we will deal with null data this way:\n\n* _Drop the null values on rate and approx_scot attributes and store it on a brand new dataset, preserving what we've got until now and using this new data to make analysis involving those attributes._\n* _Fill the location and rest_type null data with the string \"Not defined\"."},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:12.326760Z","end_time":"2019-08-14T18:18:12.376713Z"},"trusted":true},"cell_type":"code","source":"bengaluru_not_null = bengaluru.dropna(subset=['rate_transformed', 'approx_cost(for two people)'])\nbengaluru_filled = bengaluru_not_null.fillna('Not defined')\nbengaluru_filled.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Outliers"},{"metadata":{},"cell_type":"markdown","source":"As long as one of our goals is to train a Machine Learning model to predict the rate the costumer would give for a restaurant, it's important to look at the outliers of our data. Let's use some boxplot with other attributes like 'online_order' and 'book_table' to do that."},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:12.377925Z","end_time":"2019-08-14T18:18:12.599591Z"},"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(13, 6))\nax = sns.boxplot(x='online_order', y='rate_transformed', hue='book_table', data=bengaluru_filled)\nax.set_title('Rate Distribution by Online Order and Book Table Service', size=14)\nax.annotate('Some extreme \\n      outliers', xy=(1.16, 1.82), xytext=(0.4, 1.80),\n            arrowprops=dict(facecolor='black'), fontsize=14)\nax.annotate('', xy=(-0.17, 2.2), xytext=(0.35, 1.8),\n            arrowprops=dict(facecolor='black'), fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is not wrong to say that Book Table service can increase the restaurant rate. We will go deeper into this in the next steps"},{"metadata":{},"cell_type":"markdown","source":"## Graphical Analysis"},{"metadata":{},"cell_type":"markdown","source":"Until now, we worked on some data preparation in order to get a vision of what we have in hands. Now we have enough information to build a graphical analysis and look for correlations between the attributes. Let's start with the top restaurants in Bengaluru by rating."},{"metadata":{},"cell_type":"markdown","source":"### Top Restaurants by Rating"},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:12.601090Z","end_time":"2019-08-14T18:18:12.632033Z"},"trusted":true},"cell_type":"code","source":"grouped_rate = bengaluru_filled.groupby(by='name', as_index=False).mean()\ntop_rating = grouped_rate.sort_values(by='rate_transformed', ascending=False).iloc[:10, np.r_[0, -1]]\ntop_rating","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:12.633360Z","end_time":"2019-08-14T18:18:12.847678Z"},"trusted":true},"cell_type":"code","source":"# Adjusting a restaurant name\ntop_rating.iloc[1, 0] = 'Santa Spa Cuisine'\n\n# Plotting\nfig, ax = plt.subplots(figsize=(13, 5))\nax = sns.barplot(y='name', x='rate_transformed', data=top_rating, palette='Blues_d')\nax.set_xlim([4.7, 4.95])\nax.set_xlabel('Mean Rate')\nax.set_ylabel('')\nformat_spines(ax, right_border=False)\n\nfor p in ax.patches:\n    width = p.get_width()\n    ax.text(width+0.007, p.get_y() + p.get_height() / 2. + 0.2, '{:1.2f}'.format(width), \n            ha=\"center\", color='grey')\n\nax.set_title('Top 10 Restaurants in Bengaluru by Rate', size=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cost and Rate of Restaurants"},{"metadata":{},"cell_type":"markdown","source":"We have already seen the distribution of the approx_cost attribute and how it is bumpy. Now we will see if the rate of the most expensive restaurants and the rate of the ones with the lowest cost. Maybe there is a relationship between this two attributes."},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:12.849395Z","end_time":"2019-08-14T18:18:13.129483Z"},"trusted":true},"cell_type":"code","source":"high_cost = grouped_rate.sort_values(by='approx_cost(for two people)', \n                                     ascending=False).iloc[:10, np.r_[0, -1, -2]]\nfig, ax = plt.subplots(figsize=(10, 7))\nsns.barplot(x='name', y='approx_cost(for two people)', data=high_cost, ax=ax, palette='PuBu')\nax2 = ax.twinx()\nsns.lineplot(x='name', y='rate_transformed', data=high_cost, ax=ax2, color='crimson', sort=False)\nax.tick_params(axis='x', labelrotation=90)\nax.set_ylim(700, 1000)\nax2.set_ylim([3, 4.5])\nformat_spines(ax, right_border=True)\nformat_spines(ax2, right_border=True)\nax.xaxis.set_label_text(\"\")\n\nxs = np.arange(0,10,1)\nys = high_cost['rate_transformed']\n\nfor x,y in zip(xs,ys):\n    label = \"{:.2f}\".format(y)\n    plt.annotate(label, # this is the text\n                 (x,y), # this is the point to label\n                 textcoords=\"offset points\", # how to position the text\n                 xytext=(0,10), # distance from text to points (x,y)\n                 ha='center', # horizontal alignment can be left, right or center\n                 color='black')\n\nax.set_title('Higher Cost Restaurants and its Rates', size=14)\nax.set_ylabel('Mean Approx Cost')\nax2.set_ylabel('Mean Rate')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:13.130868Z","end_time":"2019-08-14T18:18:13.431929Z"},"trusted":true},"cell_type":"code","source":"low_cost = grouped_rate.sort_values(by='approx_cost(for two people)', \n                                     ascending=True).iloc[:10, np.r_[0, -1, -2]]\nfig, ax = plt.subplots(figsize=(10, 7))\nsns.barplot(x='name', y='approx_cost(for two people)', data=low_cost, ax=ax, palette='PuBu')\nax2 = ax.twinx()\nsns.lineplot(x='name', y='rate_transformed', data=low_cost, ax=ax2, color='crimson', sort=False)\nax.tick_params(axis='x', labelrotation=90)\nax.set_ylim([0, 2])\nax2.set_ylim([0, 5])\nformat_spines(ax, right_border=True)\nformat_spines(ax2, right_border=True)\nax.xaxis.set_label_text(\"\")\n\nxs = np.arange(0,10,1)\nys = low_cost['rate_transformed']\n\nfor x,y in zip(xs,ys):\n    label = \"{:.2f}\".format(y)\n    plt.annotate(label, # this is the text\n                 (x,y), # this is the point to label\n                 textcoords=\"offset points\", # how to position the text\n                 xytext=(0,10), # distance from text to points (x,y)\n                 ha='center', # horizontal alignment can be left, right or center\n                 color='black')\n\nax.set_title('Lower Cost Restaurants and its Rates', size=14)\nax.set_ylabel('Mean Approx Cost')\nax2.set_ylabel('Mean Rate')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**\n\nThe rate of a restaurants isn't defined by its cost. There are expensive restaurants with a bad average rate and there are cheap restaurants with good rate."},{"metadata":{},"cell_type":"markdown","source":"To be certain of what we just said, let's see the correlation between cost and rate of restaurants."},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:13.433317Z","end_time":"2019-08-14T18:18:13.887384Z"},"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 6))\nsns.scatterplot(x='rate_transformed', y='approx_cost(for two people)', data=bengaluru, ax=ax)\nformat_spines(ax, right_border=False)\nax.set_title('Correlation Between Rate and Approx Cost', size=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:13.888971Z","end_time":"2019-08-14T18:18:19.720262Z"},"trusted":true},"cell_type":"code","source":"# Separating by Online Order and Book Table options\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\nsns.scatterplot(x='rate_transformed', y='approx_cost(for two people)', hue='online_order', \n                data=bengaluru, ax=axs[0], palette=['navy', 'crimson'])\nsns.scatterplot(x='rate_transformed', y='approx_cost(for two people)', hue='book_table', \n                data=bengaluru, ax=axs[1], palette=['navy', 'crimson'])\nformat_spines(axs[0], right_border=False)\nformat_spines(axs[1], right_border=False)\naxs[0].set_title('Cost and Rate Distribution by Online Order Option', size=14)\naxs[1].set_title('Cost and Rate Distribution by Book Table Option', size=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusions:**\n\n* We can see that the rating of each restaurant isn't influenced by its approx cost.\n* There is a notable concentration of restaurants that don't have online order service and a low cost for two people. On the other hand, there is a concentration of restaurants on top of cost that offer online order service. Probably this service can increase the cost of restaurants;\n* Few restaurants offer book table service. In general, the ones who offer it have higher rate."},{"metadata":{},"cell_type":"markdown","source":"### Online Order and Book Table Services"},{"metadata":{},"cell_type":"markdown","source":"Above we used the Online Order and Book Table attributes to see if there was a correlation between them and rate or the approx cost of a restaurant. Let's dig a little more on these two. As long as online order and book table don't have null values, we will use the original dataset to analyze them."},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:19.721543Z","end_time":"2019-08-14T18:18:19.919845Z"},"trusted":true},"cell_type":"code","source":"online_order_labels = list(df['online_order'].value_counts().index)\nbook_table_labels = list(df['book_table'].value_counts().index)\ntext = f'Total: {len(df)}'\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 7))\ndonut_plot('online_order', axs[0], df, online_order_labels, text=text)\ndonut_plot('book_table', axs[1], df, book_table_labels, text=text, colors=['crimson', 'navy'])\n\naxs[0].set_title('Online Order Service', size=14)\naxs[1].set_title('Book Table Service', size=14)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the average cost in each case."},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:19.921642Z","end_time":"2019-08-14T18:18:19.955920Z"},"trusted":true},"cell_type":"code","source":"# Online order restaurants comparison\ndf.groupby(by='online_order').mean()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:19.958304Z","end_time":"2019-08-14T18:18:19.994018Z"},"trusted":true},"cell_type":"code","source":"# Book table restaurants comparison\ndf.groupby(by='book_table').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusions:**\n\n* Restaurants with book table service have a higher rating. Curiously, the cost of these restaurants are significantly lower.\n* Restaurants with online order service have higher approx cost (for two people). The rating of restaurantes who offer this service and those who don't are about to be the same;\n* It is possible to say that customers are more satisfied with restaurants with book table service."},{"metadata":{},"cell_type":"markdown","source":"Maybe it will be better to see this by another perspective: let's plot the proportion of the online order and book table services for each rate we've got for the restaurants."},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:19.998036Z","end_time":"2019-08-14T18:18:20.733573Z"},"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 7))\n\n# Online order by rate\nonline_order_rate = pd.crosstab(df['rate_transformed'], df['online_order'])\nonline_order_rate.div(online_order_rate.sum(1).astype(float), axis=0).plot(kind='barh', \n                                                                       stacked=True, ax=axs[0], \n                                                                       colors=['crimson', 'navy'])\nformat_spines(axs[0], right_border=False)\naxs[0].set_title('Online Order Proportion Considering Rate', size=14)\naxs[0].set_ylabel('Restaurant Rate')\n\n# Book table by rate\nbook_table_rate = pd.crosstab(df['rate_transformed'], df['book_table'])\nbook_table_rate.div(book_table_rate.sum(1).astype(float), axis=0).plot(kind='barh', \n                                                                       stacked=True, ax=axs[1], \n                                                                       colors=['crimson', 'navy'])\nformat_spines(axs[1], right_border=False)\naxs[1].set_title('Book Table Proportion Considering Rate', size=14)\naxs[1].set_ylabel('')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we said, when Book Table service is available, the restaurant rate tend to be higher. Maybe we can say the same about Online Order service, but not as huge as we have in Book Table service."},{"metadata":{},"cell_type":"markdown","source":"### Restaurants Type"},{"metadata":{},"cell_type":"markdown","source":"Let's now see the approx cost and rate of restaurants by its type."},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:20.735437Z","end_time":"2019-08-14T18:18:21.095701Z"},"trusted":true},"cell_type":"code","source":"rest_params = df.groupby(by='listed_in(type)', as_index=False).mean().sort_values(by='rate_transformed', \n                                                                                  ascending=False)\nfig, ax = plt.subplots(figsize=(10, 7))\nsns.barplot(x='listed_in(type)', y='approx_cost(for two people)', data=rest_params, ax=ax, palette='Blues_d')\nax2 = ax.twinx()\nsns.lineplot(x='listed_in(type)', y='rate_transformed', data=rest_params, ax=ax2, color='crimson', sort=False)\nax.tick_params(axis='x', labelrotation=90)\nformat_spines(ax, right_border=True)\nformat_spines(ax2, right_border=True)\nax.xaxis.set_label_text(\"\")\n\nxs = np.arange(0,10,1)\nys = rest_params['rate_transformed']\n\nfor x,y in zip(xs,ys):\n    label = \"{:.2f}\".format(y)\n    plt.annotate(label, # this is the text\n                 (x,y), # this is the point to label\n                 textcoords=\"offset points\", # how to position the text\n                 xytext=(0,10), # distance from text to points (x,y)\n                 ha='center', # horizontal alignment can be left, right or center\n                 color='black')\n\nax.set_title('Average Cost and Rating of Restaurants by Type', size=14)\nax.set_ylabel('Mean Approximated Cost')\nax2.set_ylabel('Mean Rate')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusions:**\n\n* _Pubs and bars_ and _Drinks & nightlife_ are the restaurants with lowest cost and highest rate. Customers prefer restaurants with lower cost.\n* _Delivery_ and _Dine out_ restaurants are the ones with the lowest rate. Probably this kind of restaurant has the most demanding customers.\n* _Cafe_ restaurants are the most expensive."},{"metadata":{},"cell_type":"markdown","source":"### Location"},{"metadata":{},"cell_type":"markdown","source":"Another good point to look is about the location of the restaurants and how it influences cost and rate."},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:21.097177Z","end_time":"2019-08-14T18:18:21.821800Z"},"trusted":true},"cell_type":"code","source":"city_rest = df.groupby(by='listed_in(city)', as_index=False).mean().sort_values(by='rate_transformed', \n                                                                                  ascending=False)\nfig, ax = plt.subplots(figsize=(14, 7))\nsns.barplot(x='listed_in(city)', y='approx_cost(for two people)', data=city_rest, ax=ax, palette='Blues_d')\nax2 = ax.twinx()\nsns.lineplot(x='listed_in(city)', y='rate_transformed', data=city_rest, ax=ax2, color='crimson', sort=False)\nax.tick_params(axis='x', labelrotation=90)\nformat_spines(ax, right_border=True)\nformat_spines(ax2, right_border=True)\nax.xaxis.set_label_text(\"\")\n\nxs = np.arange(0,len(city_rest),1)\nys = city_rest['rate_transformed']\n\nfor x,y in zip(xs,ys):\n    label = \"{:.2f}\".format(y)\n    plt.annotate(label, # this is the text\n                 (x,y), # this is the point to label\n                 textcoords=\"offset points\", # how to position the text\n                 xytext=(0,10), # distance from text to points (x,y)\n                 ha='center', # horizontal alignment can be left, right or center\n                 color='black')\n\nax.set_title('Average Cost and Rating of Restaurants by City', size=14)\nax.set_ylabel('Mean Approximated Cost')\nax2.set_ylabel('Mean Rate')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusions:**\n\n* Restaurants in Church Street, MG Road and Brigade Road are the customer's favorites. Its costs are low and its rates are high.\n* On the other hand, restaurants in Electronic City are the ones with lowest rate.\n* Restaurants with highest cost for two people are in Kaiyan Nagar."},{"metadata":{},"cell_type":"markdown","source":"# Explained Data Prep"},{"metadata":{},"cell_type":"markdown","source":"Well we've made this far and we have to be proud of what we've got! We went trough the attributes and got intuition about our data. Now we have a big challenge in hands: to train a Machine Learning model to predict restaurant's rate."},{"metadata":{},"cell_type":"markdown","source":"## Classes for Pipeline"},{"metadata":{},"cell_type":"markdown","source":"Remember we made some transformations on data? Here we have to map these transformartions in order to create a preparation pipeline whenever we receive new data to evaluate. It's easier to create *classes* to put all the steps and so it goes:\n\n* **AttrSelect:** responsible for selecting only the \"important\" attributes for training a model\n* **TransformData:** responsible for changing the data types and transforming the rate attribute into a number\n* **DropNull:** responsible for droping the null values on the data\n* **SplitData:** responsible for splitting the data into a train and a test set"},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:21.823609Z","end_time":"2019-08-14T18:18:21.827001Z"},"trusted":true},"cell_type":"code","source":"# Class for filtering data\nclass AttrSelect(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, cols):\n        self.cols = cols\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        \n        return X.loc[:, cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we execute the _transform_ method for this class, we will receive the original dataframe filtered by the important columns and also a y with the target. Let's see:"},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:21.828564Z","end_time":"2019-08-14T18:18:26.900012Z"},"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/zomato.csv')\ncols = ['online_order', 'book_table', 'votes', 'approx_cost(for two people)', \n                'listed_in(type)', 'listed_in(city)', 'rate']\n\nattr_selector = AttrSelect(cols=cols)\ndf_selected = attr_selector.fit_transform(df)\ndf_selected.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good! Let's keep going with the transformations. We have to create a class for threat the data types and fix the rate column."},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:26.901558Z","end_time":"2019-08-14T18:18:26.907070Z"},"trusted":true},"cell_type":"code","source":"# What we've got now\ndf_selected.dtypes","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:26.909504Z","end_time":"2019-08-14T18:18:26.922819Z"},"trusted":true},"cell_type":"code","source":"# Class for making more preparation\nclass TransformData(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, new_data=False):\n        self.new_data = new_data\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        # Transforming approx_cost column\n        X['approx_cost(for two people)'] = X['approx_cost(for two people)'].astype(str)\n        X['approx_cost(for two people)'] = X['approx_cost(for two people)'].apply(lambda x: x.replace(',', '.'))\n        X['approx_cost(for two people)'] = X['approx_cost(for two people)'].astype(float)\n        \n        if not self.new_data:        \n            # Transforming rate column\n            X['rate'] = X['rate'].astype(str)\n            X['rate'] = X['rate'].apply(lambda x: x.split('/')[0])\n            X['rate'] = X['rate'].apply(lambda x: x.replace('NEW', str(np.nan)))\n            X['rate'] = X['rate'].apply(lambda x: x.replace('-', str(np.nan)))\n            X['rate'] = X['rate'].astype(float)\n        \n        return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try this one!"},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:26.924678Z","end_time":"2019-08-14T18:18:27.079385Z"},"trusted":true},"cell_type":"code","source":"data_transformer = TransformData()\ndf_transformed = data_transformer.fit_transform(df_selected)\ndf_transformed.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:27.087067Z","end_time":"2019-08-14T18:18:27.105981Z"},"trusted":true},"cell_type":"code","source":"df_transformed.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice! The only thing left to complete our common pipeline is handling the null data."},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:27.109766Z","end_time":"2019-08-14T18:18:27.115207Z"},"trusted":true},"cell_type":"code","source":"# Class for dealing with null data\nclass DropNull(BaseEstimator, TransformerMixin):\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return X.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Go on!"},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:27.117025Z","end_time":"2019-08-14T18:18:27.140127Z"},"trusted":true},"cell_type":"code","source":"# What we've got until now\nprint(f'Data dimensions before handling null data: {df_transformed.shape}\\n')\nprint('Null data:\\n')\nprint(df_transformed.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:27.142360Z","end_time":"2019-08-14T18:18:27.182777Z"},"trusted":true},"cell_type":"code","source":"# Trying the class\nnull_dropper = DropNull()\ndf_prepared = null_dropper.fit_transform(df_transformed)\nprint(f'Data dimensions after handling null data: {df_prepared.shape}\\n')\nprint('Null data:\\n')\nprint(df_prepared.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yeah, we've lost some information for dropping the null data, but let's keep in mind that maybe this isn't the perfect approach for the problem and it's just the easiest one to tell us the correct way."},{"metadata":{},"cell_type":"markdown","source":"With this we just ended our common pipeline! Let's now start creating classes for threat categorical and numerical features in a separated way."},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:27.185287Z","end_time":"2019-08-14T18:18:27.195698Z"},"trusted":true},"cell_type":"code","source":"# Separating data into training and test\nclass SplitData(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, target_col='rate'):\n        self.target_col = target_col\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        features = X.drop([self.target_col], axis=1)\n        target = X.loc[:, [self.target_col]]\n        \n        return train_test_split(features, target, test_size=.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:27.198088Z","end_time":"2019-08-14T18:18:27.211609Z"},"trusted":true},"cell_type":"code","source":"data_splitter = SplitData()\nX_train, X_test, y_train, y_test = data_splitter.fit_transform(df_prepared)\n\nprint(f'X_train dimension: {X_train.shape}')\nprint(f'y_train dimension: {y_train.shape}')\nprint(f'\\nX_test dimension: {X_test.shape}')\nprint(f'y_test dimension: {y_test.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we won't touch the test set until we need to evaluate our model for the last time."},{"metadata":{},"cell_type":"markdown","source":"## Complete Pipeline"},{"metadata":{},"cell_type":"markdown","source":"Here we will put it all together in order to create a compact and easy-to-follow pipeline for data preparation. The goal is to receive the raw data and make all the preparations to return a dataset ready to be trained for a Machine Learning model.\n\nWe will divide the process in:\n\n* _**Common Pipeline:**_\n    - Feature selection;\n    - Data transformation;\n    - Null data handling;\n    - Splitting the data into a train and a test set\n    \n    \n* _**Numerical Pipeline:**_\n    - Standard Scaler;\n \n \n* _**Categorical Pipeline:**_\n    - One Hot Encoder;"},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:27.213159Z","end_time":"2019-08-14T18:18:27.226607Z"},"trusted":true},"cell_type":"code","source":"# Defining a pipeline\nnum_attribs = ['votes', 'approx_cost(for two people)']\ncat_attribs = ['online_order', 'book_table', 'listed_in(type)', 'listed_in(city)']\nall_attribs = num_attribs + cat_attribs\nX_num = X_train.loc[:, num_attribs]\nX_cat = X_train.loc[:, cat_attribs]\n\n# Common pipeline\ncommon_pipeline = Pipeline([\n    ('attr_selector', AttrSelect(all_attribs)),\n    ('data_transformer', TransformData()),\n    ('null_dropper', DropNull()),\n    ('data_splitter', SplitData())\n])\n\n# Numerical pipeline\nnum_pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n])\n\n# Categorical pipeline\ncat_pipeline = Pipeline([\n    ('one_hot', OneHotEncoder(sparse=False)),\n])\n\n# Full pipeline\nfull_pipeline = ColumnTransformer([\n    ('num', num_pipeline, num_attribs),\n    ('cat', cat_pipeline, cat_attribs),\n])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:27.228277Z","end_time":"2019-08-14T18:18:27.419242Z"},"trusted":true},"cell_type":"code","source":"# Applying pipelines\nX_train, X_test, y_train, y_test = common_pipeline.fit_transform(df)\n\nX_train_prepared = full_pipeline.fit_transform(X_train)\nX_test_prepared = full_pipeline.fit_transform(X_test)\n\n# Checking the columns\nX_train_prepared.shape[1] == X_test_prepared.shape[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting Restaurant's Rate"},{"metadata":{},"cell_type":"markdown","source":"Finally we can start the saga of training a model for predicting the restaurant's rate given by a customer. For the first trial, let's try a Linear Regression model and see what we've got."},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:27.420890Z","end_time":"2019-08-14T18:18:27.965934Z"},"trusted":true},"cell_type":"code","source":"df_performance = create_dataset()\nregressor = {\n    'lin': LinearRegression(),\n}\nresults = model_results(regressor, X_train_prepared, y_train, X_test_prepared, y_test, df_performance)\nresults.set_index('model', inplace=True)\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, we can try a lot of things right now. Maybe if we give some regularization to the model we can reach better results. Let's dig a little bit more and try Ridge, Lasso and Elastic Net without any configuration. After, we will analyze each one individually."},{"metadata":{},"cell_type":"markdown","source":"## Regularization"},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:27.967687Z","end_time":"2019-08-14T18:18:29.214360Z"},"trusted":true},"cell_type":"code","source":"df_performance = create_dataset()\nregressors = {\n    'lin': LinearRegression(),\n    'ridge': Ridge(),\n    'lasso': Lasso(),\n    'elastic': ElasticNet()\n}\nresults = model_results(regressors, X_train_prepared, y_train, X_test_prepared, y_test, df_performance)\nresults.set_index('model', inplace=True)\ncm = sns.light_palette(\"cornflowerblue\", as_cmap=True)\nresults.style.background_gradient(cmap=cm)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-08-13T10:19:44.252573Z","start_time":"2019-08-13T10:19:44.236350Z"}},"cell_type":"markdown","source":"There is nothing we can say right now but let's investigate more. Before that, we will define a function for calculating the rmse error for the models so we can get this value in an easier way."},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:29.215969Z","end_time":"2019-08-14T18:18:29.311797Z"},"trusted":true},"cell_type":"code","source":"ridge = Ridge(alpha=1)\nridge.fit(X_train_prepared, y_train)\nscores = cross_val_score(ridge, X_train_prepared, y_train, cv=5, scoring='neg_mean_squared_error')\nnp.sqrt(-scores).mean()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:29.314745Z","end_time":"2019-08-14T18:18:29.319120Z"},"trusted":true},"cell_type":"code","source":"def calc_rmse(model, X, y, cv=5, scoring='neg_mean_squared_error'):\n    \"\"\"\n    This functions is responsible for calculatin the root mean squared error for a model\n    \n    Input:\n        model: a regression model\n        X, y: features and target\n        cv: cross validation k fold\n        scoring: scoring to be evaluated (default: neg_mean_squared_error)\n        \n    Output:\n        rmse: root mean squared error\n    \"\"\"\n    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n    \n    return np.sqrt(-scores).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ridge"},{"metadata":{},"cell_type":"markdown","source":"\\begin{equation*}\n    \\alpha\\sum_{i=1}^{n}\\theta_i^2 \\text{ = }l_2 \\text{ penalty}\n\\end{equation*} "},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:29.321614Z","end_time":"2019-08-14T18:18:30.420555Z"},"trusted":true},"cell_type":"code","source":"# Alpha analysis\nalphas = [0.001, 0.003, 0.01, 0.03, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\nridge_scores = []\nfor a in alphas:\n    ridge_scores.append(calc_rmse(Ridge(alpha=a), X_train_prepared, y_train))\nfig, ax = plt.subplots(figsize=(10, 5))\nax = sns.lineplot(alphas, ridge_scores)\nformat_spines(ax, right_border=False)\nax.set_title('Alpha - Ridge Regression')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\\begin{equation*}\n    \\begin{cases}\n        \\alpha = 0 & \\mbox{Ridge Regression = Linear Regression (that's why we've got the same result before)} \\\\\n        \\alpha = high & \\mbox{High regularization and reduction of overfitting} \\\\\n        \\alpha = low & \\mbox{Low regularization on weights and maybe increase of overfitting}\n    \\end{cases}\n\\end{equation*}"},{"metadata":{},"cell_type":"markdown","source":"Ok, we can see that the best regularization therm for Ridge Regression is about 30. But even with this analysis, we have to say that the improvement is too little. Let's see what we can say about Lasso and Elastic Net."},{"metadata":{},"cell_type":"markdown","source":"### Lasso"},{"metadata":{},"cell_type":"markdown","source":"\\begin{equation*}\n    \\alpha\\sum_{i=1}^{n}|\\theta_i| \\text{ = }l_1 \\text{ penalty}\n\\end{equation*} "},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:30.421955Z","end_time":"2019-08-14T18:18:44.579900Z"},"trusted":true},"cell_type":"code","source":"# Alpha analysis\nalphas = [1e-6, 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3]\nlasso_scores = []\nfor a in alphas:\n    lasso_scores.append(calc_rmse(Lasso(alpha=a), X_train_prepared, y_train))\nfig, ax = plt.subplots(figsize=(10, 5))\nax = sns.lineplot(alphas, lasso_scores)\nformat_spines(ax, right_border=False)\nax.set_title('Alpha - Lasso Regression')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This proves that we can get similar results with Lasso too (even if in the first analysis Lasso performed a little worse comparing to Linear Regression and Ridge Regression). By the end, let's see the results of Elastic Net model."},{"metadata":{},"cell_type":"markdown","source":"### Elastic Net"},{"metadata":{},"cell_type":"markdown","source":"\\begin{equation*}\n    J(\\theta)=MSE(\\theta) + r\\alpha\\sum_{i=1}^n|\\theta_i|+\n              \\frac{1-r}{2}\\alpha\\sum_{i=1}^n\\theta_i^2\n\\end{equation*}"},{"metadata":{},"cell_type":"markdown","source":"Maybe it will take more time because of the l1_ratio..."},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:18:44.581497Z","end_time":"2019-08-14T18:21:10.295134Z"},"trusted":true},"cell_type":"code","source":"# Alpha analysis\nalphas = [1e-4, 3e-4, 1e-3, 3e-3, 0.01, 0.03, 0.1, 0.3, 1]\nmix_ratio = np.linspace(0.1, 0.9, 9)\nelastic_scores = []\nfor a in alphas:\n    for ratio in mix_ratio:\n        elastic_scores.append(calc_rmse(ElasticNet(alpha=a, l1_ratio=ratio), \n                                        X_train_prepared, y_train))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:21:10.296503Z","end_time":"2019-08-14T18:21:10.299292Z"},"trusted":true},"cell_type":"code","source":"print(f'The minimum score Elastic Net can reach is: {min(elastic_scores)}')\nprint(f'At array index: {elastic_scores.index(min(elastic_scores))}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The 9th element is the combination for alpha=1e-4 and l1_ratio=0.9"},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:21:10.300532Z","end_time":"2019-08-14T18:21:10.306438Z"},"trusted":true},"cell_type":"code","source":"print(f'The minimum score Lasso can reach is: {min(lasso_scores)}')\nprint(f'The minimum score Ridge can reach is: {min(ridge_scores)}')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-08-13T10:57:09.852548Z","start_time":"2019-08-13T10:57:09.844271Z"}},"cell_type":"markdown","source":"We can see that the Ridge Regression (with alpha=30) is the best model for this task. But as we said, the difference is too little and maybe we can improve the score by another way. Let's try a Random Forest Regression model."},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Regressor"},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:21:10.307832Z","end_time":"2019-08-14T18:21:14.720648Z"},"trusted":true},"cell_type":"code","source":"forest_rmse = calc_rmse(RandomForestRegressor(), X_train_prepared, y_train, cv=5)\nprint(f'Forest Regression rmse with cv=5 without any hyperparemeter configuration: {forest_rmse}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good! It's such a powerfull model. I think we can try Grid Search on this to see if we can go lower."},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:21:14.722557Z","end_time":"2019-08-14T18:23:08.525704Z"},"trusted":true},"cell_type":"code","source":"param_grid = [\n    {'n_estimators': [30, 40, 50], \n     'max_features': [15, 20, 40],\n    }\n]\n\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(X_train_prepared, y_train)\nbest_forest_rmse = np.sqrt(-grid_search.best_score_)\nbest_forest_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice! Let's look at the hyperparameter combination"},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:23:08.527561Z","end_time":"2019-08-14T18:23:08.534885Z"},"trusted":true},"cell_type":"code","source":"grid_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:23:08.536135Z","end_time":"2019-08-14T18:23:08.634595Z"},"trusted":true},"cell_type":"code","source":"# Evaluating on test set\nfinal_model = grid_search.best_estimator_\ny_pred = final_model.predict(X_test_prepared)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nprint(f'RMSE on test set: {rmse}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At the end, we couldn't get over some of Random Forest overfitting but I think we can consider this as a good result. Let's make some simulation to see the restaurants rate."},{"metadata":{},"cell_type":"markdown","source":"## Rate of New Restaurants"},{"metadata":{},"cell_type":"markdown","source":"It would be a good idea to take the original dataset and return only the new restaurants that had \"NEW\" on the rate column. We could prepare the data and predict the rate for those NEW restaurants. What do you think?"},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:23:08.636449Z","end_time":"2019-08-14T18:23:08.676645Z"},"trusted":true},"cell_type":"code","source":"new_restaurants = df.query('rate == \"NEW\"')\nnew_restaurants.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For preparing new data we will modify the pipeline."},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:23:08.680441Z","end_time":"2019-08-14T18:23:08.703888Z"},"trusted":true},"cell_type":"code","source":"# Common pipeline\nnew_data_pipeline = Pipeline([\n    ('attr_selector', AttrSelect(all_attribs)),\n    ('data_transformer', TransformData(new_data=True)),\n    ('null_dropper', DropNull()),\n])\n\nnew_restaurants_data = new_data_pipeline.fit_transform(new_restaurants)\nnew_restaurants_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OK I just realized that the NEW restaurants actually have 0 votes. Maybe it would be a problem for the algorithm unless we train it again dropping the \"vote\" column but I think this is not necessary. If we can't go through this analysis, we can create synthetic data as well."},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:23:08.705931Z","end_time":"2019-08-14T18:23:08.802741Z"},"trusted":true},"cell_type":"code","source":"# Predicting rate for new restaurants and adding a new collumn to the dataframe\nnew_restaurants_data.drop(['rate'], axis=1, inplace=True)\nnew_restaurants_prepared = full_pipeline.fit_transform(new_restaurants_data)\nnew_predictions = final_model.predict(new_restaurants_prepared)\nnew_restaurants_data['rate_predicted'] = new_predictions\nnew_restaurants_data.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-08-13T11:42:29.038081Z","start_time":"2019-08-13T11:42:29.030241Z"}},"cell_type":"markdown","source":"This is a really cool task don't you think? We predicted the rate of new restaurants in Bengaluru!"},{"metadata":{},"cell_type":"markdown","source":"# Classifying Customer Satisfaction"},{"metadata":{},"cell_type":"markdown","source":"Let's try something different here: I want to know if it's possible to transform restaurant's rate given by the costumer into groups and run a classification task for predict whenever a new restaurant would be classified as \"excellent\" (1), \"reasonable\" (2) or \"bad\" (3). "},{"metadata":{},"cell_type":"markdown","source":"For this we will create a new column following the ranges:\n\n* 0 < rate <= 3: \"bad\" (3) restaurants\n* 3 < rate <= 4: \"reasonable\" (2) restaurants\n* rate > 4: \"excelent\" (1) restaurants"},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:23:08.804553Z","end_time":"2019-08-14T18:23:08.812098Z"},"trusted":true},"cell_type":"code","source":"# Class for creating a new column on data with the restaurant evaluation\nclass CreateRestClass(BaseEstimator, TransformerMixin):\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        bin_edges = [0, 3, 4, 5]\n        bin_names = [3, 2, 1]\n        X['rest_class'] = pd.cut(X['rate'], bins=bin_edges, labels=bin_names)\n        new_X = X.drop(['rate'], axis=1)\n        \n        return new_X","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:23:08.814162Z","end_time":"2019-08-14T18:23:13.659897Z"},"trusted":true},"cell_type":"code","source":"# Reading again the data and creating a new pipeline\ndf = pd.read_csv('../input/zomato.csv')\n\n# New pipeline for this task\nclassification_task_pipeline = Pipeline([\n    ('attr_selector', AttrSelect(all_attribs)),\n    ('data_transformer', TransformData()),\n    ('class_creator', CreateRestClass()),\n    ('null_dropper', DropNull()),\n    ('data_splitter', SplitData(target_col='rest_class'))\n])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:23:13.667359Z","end_time":"2019-08-14T18:23:13.806505Z"},"trusted":true},"cell_type":"code","source":"# Preparing new data\nX_train, X_test, y_train, y_test = classification_task_pipeline.fit_transform(df)\n\ny_train.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:23:13.808210Z","end_time":"2019-08-14T18:23:13.904221Z"},"trusted":true},"cell_type":"code","source":"# Preparing the data\nX_train_prepared = full_pipeline.fit_transform(X_train)\nX_test_prepared = full_pipeline.fit_transform(X_test)\n\n# Checking the columns\nX_train_prepared.shape[1] == X_test_prepared.shape[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification Performance"},{"metadata":{},"cell_type":"markdown","source":"OK, we have already created a new classification pipeline and prepared the data. Now we will train some classification models in order to predict the class of new restaurants."},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:23:13.906140Z","end_time":"2019-08-14T18:24:21.457768Z"},"trusted":true},"cell_type":"code","source":"classifiers = {\n    'log_reg': LogisticRegression(),\n    'tree_clf': DecisionTreeClassifier(),\n    'forest_clf': RandomForestClassifier(),\n    'svm_clf': SVC(),\n}\n\ntrained_models = {}\n\nfor key, model in classifiers.items():\n    model.fit(X_train_prepared, y_train)\n    trained_models[key] = model","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:24:21.459118Z","end_time":"2019-08-14T18:28:31.004869Z"},"trusted":true},"cell_type":"code","source":"# Creating dataframe to hold metrics\nempty_performance = create_empty_classification_dataset()\n\n# Evaluating models\nclassification_performance = model_analysis_class_task(trained_models, X_train_prepared, y_train,\n                                                       X_test_prepared, y_test, empty_performance)\n\n# Result\ncm = sns.light_palette(\"cornflowerblue\", as_cmap=True)\nclassification_performance.style.background_gradient(cmap=cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not so good as we thought at the begining, but it still shines! Let's try to improve the Random Forest Classifier with GridSearchCV."},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier"},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T18:44:41.254175Z","end_time":"2019-08-14T18:57:27.289869Z"},"trusted":true},"cell_type":"code","source":"param_grid = [\n    {'n_estimators': [60, 70, 80], \n     'max_features': [15, 20, 40],\n     'max_depth': [60, 70]\n    }\n]\n\nforest_clf = RandomForestClassifier()\ngrid_search = GridSearchCV(forest_clf, param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(X_train_prepared, y_train)\ngrid_search.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice! We increased our accuracy. Finally it's time to predict the class rate of new restaurants."},{"metadata":{},"cell_type":"markdown","source":"## Class of New Restaurants"},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T19:11:34.203138Z","end_time":"2019-08-14T19:11:34.656214Z"},"trusted":true},"cell_type":"code","source":"new_restaurants = df.query('rate == \"NEW\"')\nnew_restaurants_data = new_data_pipeline.fit_transform(new_restaurants)\nfinal_model = grid_search.best_estimator_\n\n# Predicting rate for new restaurants and adding a new collumn to the dataframe\nnew_restaurants_data.drop(['rate'], axis=1, inplace=True)\nnew_restaurants_prepared = full_pipeline.fit_transform(new_restaurants_data)\nnew_predictions = final_model.predict(new_restaurants_prepared)\nnew_restaurants_data['class_predicted'] = new_predictions","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T19:11:35.509062Z","end_time":"2019-08-14T19:11:35.525389Z"},"trusted":true},"cell_type":"code","source":"# Mapping classes\nclass_dict = {\n    1: 'excelent',\n    2: 'reasonable',\n    3: 'bad'\n}\n\nnew_restaurants_data['class_predicted'] = new_restaurants_data['class_predicted'].map(class_dict)\nnew_restaurants_data.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2019-08-14T19:11:39.566932Z","end_time":"2019-08-14T19:11:39.573972Z"},"trusted":true},"cell_type":"code","source":"new_restaurants_data['class_predicted'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TODO\n\n    - Apply NLP to customer Reviews"},{"metadata":{},"cell_type":"markdown","source":"I hope you like this kernel. Please **upvote** if you do!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}