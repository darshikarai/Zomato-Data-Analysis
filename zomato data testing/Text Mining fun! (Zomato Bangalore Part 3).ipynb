{"cells":[{"metadata":{},"cell_type":"markdown","source":"Fish fry, chicken fried rice, masala dosa, paneer tikka. Can you guess which of these food items made it to the top 50 mentions in Zomato Bangalore customer reviews?\n\nThis is Part Three of my analysis of the Zomato Bangalore dataset. In Part One we explored the data and predicted restaurant ratings with six selected features using Linear Regression, while in Part Two we predicted the ratings (split into classes) using classification models. \n\nIn this kernel we will apply text mining / NLP techniques to extract insights from textual features like customer reviews. Then we will try to predict restaurant ratings by feeding the transformed text to a neural network.\n\nThis kernel consists of:\n\n- Data cleaning (identifying and dropping duplicates, selecting features)\n- Text mining and insights (ngrams, bigrams, trigrams and FreqDist plots)\n- Text processing (regex, tokenizing, stopword removal, lemmatizing, vectorizing)\n- Building an LSTM Neural Network \n- Model evaluation\n- Results summary"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport nltk\nfrom nltk.corpus import RegexpTokenizer as regextoken\nfrom nltk.corpus import stopwords\nfrom nltk import FreqDist, bigrams, trigrams\nfrom nltk import WordNetLemmatizer\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nimport gensim\nfrom gensim.models.keyedvectors import KeyedVectors\nfrom keras.models import Sequential\nfrom keras.models import Model\nfrom keras.layers import Dense, Flatten, Embedding, Conv1D, MaxPooling1D, Dropout, LSTM, GRU\nfrom keras.regularizers import l1, l2\nfrom sklearn.metrics import classification_report\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nzomato = pd.read_csv(\"../input/zomato-bangalore-restaurants/zomato.csv\", na_values = [\"-\", \"\"])\n# Making a copy of the data to work on\ndata = zomato.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping duplicates - see Part One for explanation\n\ngrouped = data.groupby([\"name\", \"address\"]).agg({\"listed_in(type)\" : list})\nnewdata = pd.merge(grouped, data, on = ([\"name\", \"address\"]))\nnewdata[\"listed_in(type)_x\"] = newdata[\"listed_in(type)_x\"].astype(str) # converting unhashable list to a hashable type\nnewdata.drop_duplicates(subset = [\"name\", \"address\", \"listed_in(type)_x\"], inplace = True)\nnewdata = newdata.reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transforming the ratings column \n\nnewdata[\"rating\"] = newdata[\"rate\"].str[:3] # Extracting the first three characters of each string in \"rate\"\n# Removing rows with \"NEW\" in ratings as it is not a predictable level\nnewdata = newdata[newdata.rating != \"NEW\"] \n# Dropping rows that have missing values in ratings \nnewdata = newdata.dropna(subset = [\"rating\"])\n# Converting ratings to a numeric column so we can discretize it\nnewdata[\"rating\"] = pd.to_numeric(newdata[\"rating\"])\n# Discretizing the ratings into a categorical feature with 4 levels\n\nnewdata[\"rating\"] = pd.cut(newdata[\"rating\"], bins = [0, 3.0, 3.5, 4.0, 5.0], labels = [\"0\", \"1\", \"2\", \"3\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our four rating bins (classes) will be 0 to 3 < 3 to 3.5 < 3.5 to 4 < 4 to 5. To make label encoding easier later, we'll label these classes 0, 1, 2, 3. We can think of these as Very Low, Low, Medium and High."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing the rating class distribution\nplt.figure(figsize = (10, 5))\nsns.countplot(newdata[\"rating\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary statistics\nnewdata.describe(include = \"all\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Agenda\nWe will use the reviews_list, menu_item, dish_liked and cuisines columns for our analysis.\n\nFirst, we will look at the customer reviews and pull out the most common words and phrases. Next, we will analyse cuisine listings and identify cuisines that are rare in Bangalore. Finally we will build a neural network with all four features to predict restaurant ratings."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a new dataset that has only customer reviews and restaurant ratings\nreviews_data = newdata[[\"reviews_list\", \"rating\"]]\n# Examining the reviews for the first restaurant in the dataset\nreviews_data[\"reviews_list\"][0]\n# The text needs cleaning up","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting all the text to lowercase\nreviews_data[\"reviews_list\"] = reviews_data[\"reviews_list\"].apply(lambda x: x.lower())\n\n# Creating a regular expression tokenizer that matches only alphabets\n# This will return separate words (tokens) from the text\ntokenizer = regextoken(\"[a-zA-Z]+\") \n# Applying the tokenizer to each row of the reviews\nreview_tokens = reviews_data[\"reviews_list\"].apply(tokenizer.tokenize)\n# Examining the tokens created for the first row / restaurant\nprint(review_tokens[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing and examining the English stopwords directory \n# These are common words that typically don't add meaning to the text and can be removed\nstop = stopwords.words(\"english\")\nprint(stop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding custom words to stopwords \nstop.extend([\"rated\", \"n\", \"nan\", \"x\"])\n# Removing stopwords from the tokens\nreview_tokens = review_tokens.apply(lambda x: [token for token in x if token not in stop])\n# Concatenating all the reviews \nall_reviews = review_tokens.astype(str).str.cat()\ncleaned_reviews = tokenizer.tokenize(all_reviews)\n\n# Getting the frequency distribution of individual words in the reviews\nfd = FreqDist()\nfor word in cleaned_reviews:\n    fd[word] += 1\n    \n# Examining the top 5 most frequent words\nfd.most_common(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the top 50 most frequent words\nplt.figure(figsize = (10, 5))\nfd.plot(50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\nOf the 50 most frequent words across customer reviews, six reveal food preferences: **chicken, biryani, veg, pizza, rice, paneer**. The only negative word in the top 50 is \"bad\".\n\nFactors contributing to restaurant experience are mentioned in the following (descending) order of frequency: place > taste > service > time > ambience > staff > quality > delivery > menu > quantity > friendly.\n\nNow let us repeat the analysis on a bi-gram level. Bi-grams are pairs of words which can provide better context than individual words."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating bigrams from the reviews\nbigrams = bigrams(cleaned_reviews)\n# Getting the bigram frequency distribution\nfd_bigrams = FreqDist()\nfor bigram in bigrams:\n    fd_bigrams[bigram] += 1\n# Examining the top 5 most frequent bigrams\nfd_bigrams.most_common(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the top 50 most frequent bigrams\nplt.figure(figsize = (10, 5))\nfd_bigrams.plot(50)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\n\nWe have some new insights! Food items/preferences mentioned in the top 50 bigrams are **ice cream, non veg, North Indian, chicken biryani, fried rice, chicken and South Indian**. Top six bigrams related to restaurant experience: good food > good place > good service > value (for) money > pocket friendly > ambience good. \n\nThere's a key insight here: **the expense factor, which was missed by individual word frequency counts, was picked up by the bigram frequency counts.**\n\nZomato might also be happy to know their membership program \"Zomato Gold\" is in the top 50 bigrams, with 2593 mentions in the customer reviews.\n\nWhat about trigrams? "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating trigrams from the reviews\ntrigrams = trigrams(cleaned_reviews)\n\nfd_trigrams = FreqDist()\nfor trigram in trigrams:\n    fd_trigrams[trigram] += 1\n\nfd_trigrams.most_common(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 5))\nfd_trigrams.plot(50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\nThere appears to be some bad data (strings of \"xa xa xa\") somewhere in the reviews, but we'll ignore that. The specific food preferences we can see here are **paneer butter masala, chicken fried rice, chicken biryani, peri peri chicken and chicken ghee roast**. Bangalore is really into chicken.\n\nOn restaurant experience: a specific insight revealed by the trigrams is that **many people are looking for places to hang out with their friends**. \n\nWe also see a variety of positive trigrams like \"must visit place\", \"food really good\", \"service also good\" and \"worth every penny\". However, there is only one negative trigram in the top 50 - \"worst food ever\".\n\nWe now have plenty of insights into customer preferences and experiences, and will move onto an analysis of Bangalore's cuisines."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a new dataset with cuisines info and restaurant ratings\ncuisines = newdata[[\"cuisines\", \"rating\"]]\ncuisines[\"cuisines\"] = cuisines[\"cuisines\"].astype(str)\n# Converting to lowercase\ncuisines[\"cuisines\"] = cuisines[\"cuisines\"].apply(lambda x: x.lower())\n# Tokenizing the cuisines\ncuisine_tokens = cuisines[\"cuisines\"].apply(tokenizer.tokenize)\n# Concatenating all the cuisine names into one text document\nall_cuisines = cuisine_tokens.astype(str).str.cat()\ncleaned_cuisines = tokenizer.tokenize(all_cuisines)\n\n# Generating cuisine frequencies \nfd_cuisine = FreqDist()\nfor cuisine in cleaned_cuisines:\n    fd_cuisine[cuisine] += 1\n    \n# Printing the 50 most common cuisines (top 50)\nprint(fd_cuisine.most_common(50))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\nOne must be careful when interpreting these lists. For example, \"dogs\" can't be a cuisine but the preceding word \"hot\" tells us that the cuisine is \"hot dogs\". Another tricky one is Cantonese, which comes under Chinese and so might not really be rare.\n\nWe've done our reviews and cuisines analysis and will now prepare all the text in the dataset for feeding into a neural network."},{"metadata":{},"cell_type":"markdown","source":"## Text Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting all the text to strings\nnewdata[[\"reviews_list\", \"menu_item\", \"dish_liked\", \"cuisines\"]] = newdata[[\"reviews_list\", \"menu_item\", \"dish_liked\", \"cuisines\"]].astype(\"str\")\n# Combining all the text data into a single feature called \"text\"\nnewdata[\"text\"] = newdata[\"reviews_list\"] + \" \" + newdata[\"menu_item\"] + \" \" + newdata[\"dish_liked\"] + \" \" + newdata[\"cuisines\"]\n# Creating a new dataset with text and restaurant ratings\ntext_data = newdata[[\"text\", \"rating\"]]\n# Converting text to lowercase\ntext_data[\"text\"] = text_data[\"text\"].apply(lambda x: x.lower())\n# Tokenizing the text\ntokens = text_data[\"text\"].apply(tokenizer.tokenize) \n# Removing stopwords \ntokens = tokens.apply(lambda x: [token for token in x if token not in stop])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tokens[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Writing a function to lemmatize words\nlmtzr = WordNetLemmatizer()\ndef lem(text):\n    return [lmtzr.lemmatize(word) for word in text]\n\n# Applying the function to each row of the text\n# i.e. reducing each word to its lemma\ntokens_new = tokens.apply(lem)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying label encoding and one hot encoding to the restaurant rating classes \nle = LabelEncoder()\ntarget = le.fit_transform(text_data[\"rating\"])\ntarget = to_categorical(target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data into train and test sets (stratified)\nX_train, X_test, y_train, y_test = train_test_split(tokens_new, target, test_size = 0.3, random_state = 0, stratify = target)\n\n# Processing the text with the Keras tokenizer\nt = Tokenizer() \nt.fit_on_texts(X_train)\n# Setting a vocabulary size that we will specify in the neural network\nvocab_size = len(t.word_index) + 1\n# The t.word_index contains each unique word in our text and an integer assigned to it\nprint(vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding the text as sequences of integers\ntrain_sequences = t.texts_to_sequences(X_train)\ntest_sequences = t.texts_to_sequences(X_test)\n# Adding zeros so each sequence has the same length \ntrain_padded = pad_sequences(train_sequences, maxlen=500)\ntest_padded = pad_sequences(test_sequences, maxlen=500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word embedding \nWe'll use Google's pre-trained Word2Vec word embeddings."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading Word2Vec word embeddings \n\nword_vectors = KeyedVectors.load_word2vec_format('../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin', binary=True)\n\nembedding_dim = 300 # each word will become a 300-d vector\n\n# Creating an empty matrix \nembedding_matrix = np.zeros((vocab_size, embedding_dim)) \n# Each row is a word with 300 dimensions\n\n# Populating the matrix\nfor word, i in t.word_index.items(): # for each word in the customer reviews vocabulary\n    try:\n        # get the Word2Vec vector representation for that word\n        embedding_vector = word_vectors[word] \n        # add it to the embedding matrix\n        embedding_matrix[i] = embedding_vector \n        # handle new words by generating random vectors for them\n    except KeyError: \n        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25), embedding_dim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Examining the words embeddings - vector representations of words\nembedding_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building an LSTM neural network\n\nwarnings.filterwarnings(\"ignore\")\nmax_length = 500 # maximum length of each input string (movie review)\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embedding_dim, input_length=max_length, weights = [embedding_matrix], trainable = False))\nmodel.add(LSTM(100, activation = \"tanh\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(4, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer = \"adam\", metrics=['accuracy'])\nmodel.fit(train_padded, y_train, validation_data=(test_padded, y_test), epochs=15, batch_size=512)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting on the train data\npred_train = model.predict(train_padded)\npred_train = np.argmax(pred_train, axis=1)\ny_train = np.argmax(y_train, axis=1)\n# Printing evaluation metrics\nprint(classification_report(y_train, pred_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting on the test data\npred_test = model.predict(test_padded)\npred_test = np.argmax(pred_test, axis=1)\ny_test = np.argmax(y_test, axis = 1)\n# Printing evaluation metrics\nprint(classification_report(y_test, pred_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results summary\nBy applying text mining techniques to customer reviews and other data, **we discovered common food preferences which became more specific as we progressed (dish names)**. **We also noted which aspects of a restaurant people care about and in what order of priority. Then we identified the most and least common cuisines in the city along with their prevalence.**\n\nThe text mining activity was interesting from both a data science perspective and a business perspective, as it showed the usefulness of different NLTK tools and revealed actionable insights.\n\nAfter processing the text, we fed it to an LSTM network with pre-trained Word2Vec word vectors to see if we could predict the four rating classes we created. Accuracy and average F1 scores were lower than what we got with XGBoost in Part Two of my analysis. \n\nThe conclusion from this three-part analysis is that non-text features and tree-based classification models do a better job of predicting the restaurant ratings than LSTM with text features.\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}